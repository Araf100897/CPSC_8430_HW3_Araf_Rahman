{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e604b150-2b2d-49c9-9cc6-c71985669298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 20:00:39.443455: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-08 20:00:39.461506: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-08 20:00:39.466975: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-08 20:00:39.480548: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-08 20:00:41.702706: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1427fab2-53fe-4cbd-a958-7c075462cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for data preprocessing and answer positioning\n",
    "\n",
    "def preprocess_json(path):\n",
    "    \"\"\"Loads a JSON file and extracts contexts, questions, and answers, adjusting answer indices if needed.\"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "    \n",
    "    contexts, questions, answers = [], [], []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                access = 'plausible_answers' if 'plausible_answers' in qa else 'answers'\n",
    "                for answer in qa[access]:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "    \n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "        \n",
    "        # Adjust answer indices if necessary\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        else:\n",
    "            for n in [1, 2]:\n",
    "                if context[start_idx-n:end_idx-n] == gold_text:\n",
    "                    answer['answer_start'] = start_idx - n\n",
    "                    answer['answer_end'] = end_idx - n\n",
    "    return contexts, questions, answers\n",
    "\n",
    "\n",
    "def add_token_positions(encodings, answers):\n",
    "    \"\"\"Adds token-based start and end positions to encodings, handling cases where answers are truncated.\"\"\"\n",
    "    start_positions, end_positions = [], []\n",
    "    for i in range(len(answers)):\n",
    "        start = encodings.char_to_token(i, answers[i]['answer_start'])\n",
    "        end = encodings.char_to_token(i, answers[i]['answer_end'])\n",
    "        \n",
    "        if start is None:\n",
    "            start = tokenizer.model_max_length\n",
    "        shift = 1\n",
    "        while end is None:\n",
    "            end = encodings.char_to_token(i, answers[i]['answer_end'] - shift)\n",
    "            shift += 1\n",
    "        start_positions.append(start)\n",
    "        end_positions.append(end)\n",
    "    \n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aec29f01-8a6e-4404-9f05-35a90547490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "train_contexts, train_questions, train_answers = preprocess_json('squad/spoken_test-v1.1.json')\n",
    "val_contexts, val_questions, val_answers = preprocess_json('squad/spoken_train-v1.1.json')\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model_name = \"rein5/bert-base-uncased-finetuned-spoken-squad\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize and add positions for answers\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244db154-f880-4f1a-8824-0fa449a8b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset and DataLoader classes\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "078ee334-e791-49c1-b6a3-259f509590de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Build datasets and dataloaders\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Set up device and optimizer\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe64bd8b-2483-43ff-84d0-42d2d6ebe597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 993/993 [07:54<00:00,  2.09it/s, loss=2.09]\n",
      "Epoch 1: 100%|██████████| 993/993 [07:54<00:00,  2.09it/s, loss=1.95] \n",
      "Epoch 2: 100%|██████████| 993/993 [07:54<00:00,  2.09it/s, loss=1.37] \n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9836b95-16c8-4111-9d14-1d7f1df98e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/distilbert-custom2/tokenizer_config.json',\n",
       " 'models/distilbert-custom2/special_tokens_map.json',\n",
       " 'models/distilbert-custom2/vocab.txt',\n",
       " 'models/distilbert-custom2/added_tokens.json',\n",
       " 'models/distilbert-custom2/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "os.makedirs('models', exist_ok=True)\n",
    "model_path = 'models/distilbert-custom2'\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9891c11c-0738-471b-b353-f639e93acbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles, and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    return (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "def evaluate(gold_answers, predictions):\n",
    "    total, f1, exact_match = 0, 0, 0\n",
    "    for ground_truths, prediction in zip(gold_answers, predictions):\n",
    "        total += 1\n",
    "        exact_match += exact_match_score(prediction, ground_truths)\n",
    "        f1 += f1_score(prediction, ground_truths)\n",
    "    return {'exact_match': 100 * exact_match / total, 'f1': 100 * f1 / total}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f05818c-fa55-4d9c-b104-f9b6d014ee9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2320/2320 [06:41<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 69.82296354180701, 'f1': 77.08683807250743}\n"
     ]
    }
   ],
   "source": [
    "# Validation Loop\n",
    "model.eval()\n",
    "answers, references = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "        \n",
    "        for i in range(start_pred.shape[0]):\n",
    "            all_tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][i])\n",
    "            answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(all_tokens[start_pred[i]: end_pred[i]+1]))\n",
    "            ref = tokenizer.decode(tokenizer.convert_tokens_to_ids(all_tokens[start_true[i]: end_true[i]+1]))\n",
    "            answers.append(answer)\n",
    "            references.append(ref)\n",
    "\n",
    "# Evaluate performance\n",
    "metrics = evaluate(references, answers)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f41015-75b2-4073-a1f0-644ce421a586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
